setwd("/Users/ewengallic/Documents/Projets/TwitteR/insurance")

library(twitteR)
library(stringr)
reqURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "http://api.twitter.com/oauth/access_token"
authURL <- "http://api.twitter.com/oauth/authorize"
consumerKey <- ""
consumerSecret <- ""
twitCred <- OAuthFactory$new(consumerKey=consumerKey,
                             consumerSecret=consumerSecret,
                             requestURL=reqURL,
                             accessURL=accessURL,
                             authURL=authURL)
twitCred$handshake()
registerTwitterOAuth(twitCred)

save(twitCred,file="cred.RData")

load("cred.RData")

registerTwitterOAuth(twitCred)


#################################
# RECUPERATION RAPIDE DE TWEETS #
#################################

# Recuperer 1000 tweets contenant le mot "insurance"
tweets_insurance <- searchTwitter("insurance", n = 2000)
save(tweets_insurance, file="tweets_insurance.RData")


# Pour gerer les smileys debiles
# http://gastonsanchez.com/blog/how-to/2012/05/29/Catching-errors-when-using-tolower.html
tryTolower = function(x)
{
  # create missing value
  # this is where the returned value will be
  y = NA
  # tryCatch error
  try_error = tryCatch(tolower(x), error = function(e) e)
  # if not an error
  if (!inherits(try_error, "error"))
    y = tolower(x)
  return(y)
}

tweet <- tweets_insurance[[479]]

extract_words <- function(tweet){
  tweet_df <- as.data.frame(tweet)
  
  if(tweet_df$isRetweet){
    # Il s'agit d'un RT
    return(NULL)
  }else{
    # Il ne s'agit pas d'un RT
    
    # Mais regardons s'il s'agit d'un RT "a la main"
    
    # Recuperer les mots
    mots <- str_split(tweet_df$text, "[[:space:]]")[[1]]

    if(mots[[1]] == "RT"){
      # Il s'agit d'un RT a la main
      return(NULL)
    }else{
      # Il ne s'agit pas d'un RT a la main
      # Retirer les liens, les mentions (@), les RT, via, etc.
      res <- mots[!str_detect(mots, "^http|^@|^\\(?RT|^\\(?HT|^\\(?via|=&|\n|\"|--|^-$|^:$|&amp")]
      # Retirer la ponctuation et les parentheses
      res <- str_replace_all(res, ",|;|\\?|:|!|\\.|\\(|\\)", "")
      res <- res[str_length(res)!=0]
      
      return(res)
    }
  } 
}# End of extract_words()


# On recupere tous les mots
tweets_mots <- lapply(tweets_insurance, extract_words)
tweets_mots <- tweets_mots[!unlist(lapply(tweets_mots, is.null))]


mots <- unlist(tweets_mots)
mots <- mots[!is.na(mots)]

library(plyr)
mots <- data.frame(word = mots)
# On met en minuscule
mots$word <- sapply(mots$word, tryTolower)

# Compter le nombre d'occurence de chaque mot
mots <- count(mots)
# Puis ranger par ordre decroissant de la frequence d'apparition
mots <- arrange(mots, desc(freq))
head(mots, 50)

# Retirons le mot "insurance"
mots <- mots[-which(mots$word == "insurance"),]

# Admettons qu'on se limite au 50 premiers mots
(mots_50 <- mots[1:50,"word"])


# Pour chaque tweet, on retourne un vecteur
# donc chaque element correspond a un des 50 mots
# et valant 1 si le mot est trouve, 0 sinon
compte_50 <- lapply(tweets_mots, function(tweet){
  as.numeric(mots_50 %in% tweet)
})
compte_50 <- do.call("rbind", compte_50)
colnames(compte_50) <- mots_50

# On va parcourir la matrice 50 x 50
# pour compter pour chaque couple de mots-cles
# le nombre de fois dans lesquels ils apparaissent ensemble dans
# le corpus de tweets
couples_mots_50 <- matrix(,nrow = ncol(compte_50), ncol = ncol(compte_50))
colnames(couples_mots_50) <- rownames(couples_mots_50) <- mots_50
for(i in 1:ncol(compte_50)){
  for(j in i:ncol(compte_50)){
    if(i==j){
      couples_mots_50[i,j] <- NA
    }else{
      # Pour le mot i et le mot j, on regarde, pour chaque tweet
      # ceux qui apparaissent ensemble (la somme fait 2)
      couples_mots_50[i,j] <- couples_mots_50[j,i] <- length(which(compte_50[,i] + compte_50[,j] == 2))
    }
  }
}


heatmap(couples_mots_50, symm = TRUE)



##########################################
# POUR RECUPERER LES TWEETS EN STREAMING #
##########################################

# On track un terme, et on sauvegarde les tweets

library(streamR)
# timeout : longueur max de maintien de la connexion, en secondes
filterStream(file.name = "flu.json",
             oauth=twitCred, verbose = TRUE, timeout = 60*60,
             track = c("flu", "influenza"))

# Puis on les importe en memoire

tweets_df <- parseTweets("flu.json")

# On retire les RT
tweets_df <- tweets_df[which(!str_sub(tweets_df$text, 1, 2) %in% c("RT", "MT")),]

library(tm)


# Retirer les liens et les mentions
removeMentionsLinks <- function(x){
  str_replace_all(x, "@.*? |@.*?$|http.*? |http.*?$", "")
}

tweets_df$text <- sapply(tweets_df$text, removeMentionsLinks)

tweets <- Corpus(VectorSource(unique(tweets_df$text)))
tweets <- tm_map(tweets, stripWhitespace)
tweets <- tm_map(tweets, content_transformer(tryTolower))
# tweets <- tm_map(tweets, content_transformer(removeMentionsLinks))
myStopwords <- c(stopwords('english'), "via")
tweets <- tm_map(tweets, removeWords, myStopwords)
tweets <- tm_map(tweets, removePunctuation)
tweets <- tm_map(tweets, stemDocument)
tweets <- tm_map(tweets, stripWhitespace)
inspect(tweets[1:5])

tweets_tdm <- TermDocumentMatrix(tweets, control = list(minWordLength = 1))
inspect(tweets_tdm)
# Se limiter a une liste de mots revenant souvent
mots_frequents <- findFreqTerms(tweets_tdm, lowfreq=5)


# On transforme en matrice
tweets_m <- as.matrix(tweets_tdm)
tweets_m <- tweets_m[match(mots_frequents, rownames(tweets_m)),]


retirer <- c("influenza", "flu")
tweets_m <- tweets_m[!rownames(tweets_m) %in% retirer,]



# On va parcourir la matrice n x n (avec n le nombre de mots d'interet)
# pour compter pour chaque couple de mots-cles
# le nombre de fois dans lesquels ils apparaissent ensemble dans
# le corpus de tweets
n <- length(mots_frequents) - length(retirer)
couples_mots <- matrix(,nrow = n, ncol = n)
colnames(couples_mots) <- rownames(couples_mots) <- mots_frequents[-which(mots_frequents %in% retirer)]
for(i in 1:nrow(tweets_m)){
  for(j in i:nrow(tweets_m)){
    if(i==j){
      couples_mots[i,j] <- NA
    }else{
      # Pour le mot i et le mot j, on regarde, pour chaque tweet
      # ceux qui apparaissent ensemble
      couples_mots[i,j] <- couples_mots[j,i] <- length(which(tweets_m[i,] > 0 & tweets_m[j,] > 0))
    }
  }
}


heatmap(couples_mots, symm = TRUE)



# Pour chaque mot, 
mots_frequents_assoc <- findAssocs(tweets_tdm, mots_frequents,0)



tweets_tdm_common = removeSparseTerms(tweets_tdm, 0.01)
tweets_tdm_dense <- as.matrix(tweets_tdm_common)
library(reshape2)
tweets_tdm_dense = melt(tweets_tdm_dense, value.name = "count")

# extract_words_stream <- function(tweet){
#   if(str_sub(tweet, 1, 2) == "RT"){
#     # Il s'agit d'un RT
#     return(NULL)
#   }else{
#     # Il ne s'agit pas d'un RT
#     
#     # Recuperer les mots
#     mots <- str_split(tweet, "[[:space:]]")[[1]]
#     
#     # Retirer les liens, les mentions (@), les RT, via, etc.
#     res <- mots[!str_detect(mots, "^http|^@|^\\(?RT|^\\(?HT|^\\(?via|=&|\n|\"|--|^-$|^:$|&amp")]
#     # Retirer la ponctuation et les parentheses
#     res <- str_replace_all(res, ",|;|\\?|:|!|\\.|\\(|\\)", "")
#     res <- res[str_length(res)!=0]
#     
#     return(res)
#   }
# }# End of extract_words_stream()
# 
# # On recupere tous les mots
# tweets_mots <- lapply(tweets_df$text, extract_words_stream)
# tweets_mots <- tweets_mots[!unlist(lapply(tweets_mots, is.null))]
# 
# 
# mots <- unlist(tweets_mots)
# mots <- mots[!is.na(mots)]
# 
# library(plyr)
# mots <- data.frame(word = mots)
# # On met en minuscule
# mots$word <- sapply(mots$word, tryTolower)
# 
# # Compter le nombre d'occurence de chaque mot
# mots <- count(mots)
# # Puis ranger par ordre decroissant de la frequence d'apparition
# mots <- arrange(mots, desc(freq))
# head(mots, 50)
# 
# # Retirons le mot "insurance"
# mots <- mots[-which(mots$word == "insurance"),]
# 
# # Admettons qu'on se limite au 50 premiers mots
# (mots_50 <- mots[1:50,"word"])
# 
# 
# # Pour chaque tweet, on retourne un vecteur
# # donc chaque element correspond a un des 50 mots
# # et valant 1 si le mot est trouve, 0 sinon
# compte_50 <- lapply(tweets_mots, function(tweet){
#   as.numeric(mots_50 %in% tweet)
# })
# compte_50 <- do.call("rbind", compte_50)
# colnames(compte_50) <- mots_50
# 
# # On va parcourir la matrice 50 x 50
# # pour compter pour chaque couple de mots-cles
# # le nombre de fois dans lesquels ils apparaissent ensemble dans
# # le corpus de tweets
# couples_mots_50 <- matrix(,nrow = ncol(compte_50), ncol = ncol(compte_50))
# colnames(couples_mots_50) <- rownames(couples_mots_50) <- mots_50
# for(i in 1:ncol(compte_50)){
#   for(j in i:ncol(compte_50)){
#     if(i==j){
#       couples_mots_50[i,j] <- NA
#     }else{
#       # Pour le mot i et le mot j, on regarde, pour chaque tweet
#       # ceux qui apparaissent ensemble (la somme fait 2)
#       couples_mots_50[i,j] <- couples_mots_50[j,i] <- length(which(compte_50[,i] + compte_50[,j] == 2))
#     }
#   }
# }
# 
# 
# heatmap(couples_mots_50, symm = TRUE)



